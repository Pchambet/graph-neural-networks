{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae65c922",
   "metadata": {},
   "source": [
    "# Theory of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a42bd",
   "metadata": {},
   "source": [
    "What have seen the theory of a neuron through the perceptron model. \n",
    "The perceptron model is a simple model of a neuron that takes multiple inputs, applies weights to them, and produces an output based on a threshold function. \n",
    "We saw that the perceptron can be used to classify data into two categories.\n",
    "\n",
    "As soon as we tackle more sophisticated problems, this kind of model quickly shows its limits — it's simply not powerful enough to deliver good results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95c412",
   "metadata": {},
   "source": [
    "## Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350d717",
   "metadata": {},
   "source": [
    "To overcome this, what we traditionally do in machine learning is improve the model by adding, for example, squared variables like $x_1^2$ and $x_2^2$. This allows us to build a polynomial model — a process known as *feature engineering*. Essentially, it's the art of crafting new input features from the existing ones, and it can require a lot of time, intuition, and expertise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0a28b",
   "metadata": {},
   "source": [
    "## Deep Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60911e",
   "metadata": {},
   "source": [
    "But we are doing deep learning and not machine learning.\n",
    "\n",
    "In deep learning, we are asking ourselves the question : **What is going on if we link this neuron with other neurons?**\n",
    "\n",
    "Instead of manually crafting features, we let the model learn them automatically. The idea is to let the machine learn how to do its own feature engineering.  We do this by stacking multiple layers of neurons on top of each other, creating a deep neural network. Each layer learns to extract increasingly complex features from the input data, allowing the model to capture intricate patterns and relationships.\n",
    "\n",
    "This is the essence of deep learning: using multiple layers of neurons to learn hierarchical representations of data. Each layer transforms the input data into a more abstract representation, enabling the model to learn complex functions and make accurate predictions.\n",
    "\n",
    "Sounds incredible and hard, right? So let's fight fire with fire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea7f57",
   "metadata": {},
   "source": [
    "> We are doing a neural network from scratch, using mathematics, matrix calculations, and 30min. Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90698209",
   "metadata": {},
   "source": [
    "# Let's go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb0ba2",
   "metadata": {},
   "source": [
    "We aren't here to waste out time. So let's take the previous neuron that we built, and duplicate this bad boy.\n",
    "\n",
    "Let's call the first neuron $N_1$ and the second one $N_2$, and put them together in a layer.\n",
    "\n",
    "### Layer\n",
    "We can think of a layer as a collection of neurons that work together to process the input data. \\\n",
    "Each neuron in the layer receives the same input, applies its own transformation, and produces an output.\\\n",
    "So there are as many outputs as there are neurons in the layer. \\\n",
    "In this case, we have two neurons in the layer, $N_1$ and $N_2$, so we have two outputs $z_1$ and $z_2$.\n",
    "\n",
    "### Neuron\n",
    "Each neuron in the layer is tranforming the input data, so for each neuron in the layer, we have a weight vector $w$ and a bias $b$.\n",
    "By taking the same neurons model we had before, we can write the output of each neuron as:\n",
    "$$\n",
    "z = W \\cdot x + b\n",
    "$$\n",
    "Where $W$ is the weight vector, $x$ is the input vector, and $b$ is the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a258603",
   "metadata": {},
   "source": [
    "### Output Equation: The same but with matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a16b0",
   "metadata": {},
   "source": [
    "The same input data $x = (x_1, x_2)$ are going through both neurons at the same time, so we have two outputs $z_1$ and $z_2$.\n",
    "\n",
    "We can write this as a matrix multiplication.\n",
    "$$\n",
    "Z = Wx + b\n",
    "$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "W_{11} & W_{12} \\\\\n",
    "W_{21} & W_{22}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $W_{ij}$ is the weight of the $i$-th neuron for the $j$-th feature, and $b_i$ is the bias of the $i$-th neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10b969",
   "metadata": {},
   "source": [
    "We now recover the same equation we found for the neuron in the previous notebook — but this time, we have two neurons working in parallel.\n",
    "\n",
    "To confirm this, let's look at the expanded equation for neuron $N_1$:\n",
    "$$\n",
    "z_1 = W_{11} \\cdot x_1 + W_{12} \\cdot x_2 + b_1\n",
    "$$\n",
    "\n",
    "And for neuron $N_2$:\n",
    "$$\n",
    "z_2 = W_{21} \\cdot x_1 + W_{22} \\cdot x_2 + b_2\n",
    "$$\n",
    "\n",
    "We clearly see that both neurons are working in parallel, each with its own weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e62393",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "Now, we need to apply an activation function to the output of each neuron. \\\n",
    "Remember, The activation function is transforms the output of the neuron to introduce non-linearity into the model. \\\n",
    "We can use the same activation function for both neurons, or we can use different activation functions. \\\n",
    "In this case, we will use the sigmoid activation function for both neurons.\n",
    "$$\n",
    "a(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "Where $a(z)$ is the activation function, and $z$ is the output of the neuron.\n",
    "We can write this as:\n",
    "$$\n",
    "A = a(Z)\n",
    "$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\\n",
    "a_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a(z_1) \\\\\n",
    "a(z_2)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{1 + e^{-z_1}} \\\\\n",
    "\\frac{1}{1 + e^{-z_2}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $a_1$ and $a_2$ are the outputs of the activation function for neurons $N_1$ and $N_2$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e60e9",
   "metadata": {},
   "source": [
    "### Very important\n",
    ">The output of the activation function is the final output of the layer. \\\n",
    "In this case, we have two outputs $a_1$ and $a_2$, which are the outputs of the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3079a3",
   "metadata": {},
   "source": [
    "### Do you see why we use matrix?\n",
    "What's stopping us from adding a new row in the matrix? \\\n",
    "Creating a new neuron means adding a new row in the weight matrix and a new bias. \\\n",
    "\n",
    "Like this:\n",
    "$$\n",
    "\n",
    "\\begin{bmatrix}\n",
    "W_{11} & W_{12} \\\\\n",
    "W_{21} & W_{22} \\\\\n",
    "W_{31} & W_{32}\n",
    "\\end{bmatrix}  \n",
    ", and \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output equation didn't change:\n",
    "$$\n",
    "Z = Wx + b\n",
    "$$\n",
    "\n",
    "But if developp this, we have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "z_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "W_{11} & W_{12} \\\\\n",
    "W_{21} & W_{22} \\\\\n",
    "W_{31} & W_{32}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $W_{ij}$ is the weight of the $i$-th neuron for the $j$-th feature, and $b_i$ is the bias of the $i$-th neuron.\n",
    ">So we can add as many neurons as we want in the layer, and the output equation will remain the same.\n",
    "\n",
    "### A layer is a collection of neurons that work together to process the input data: it can be seen as a huge and complex neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c7ce3",
   "metadata": {},
   "source": [
    "## Adding a second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73aa2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
